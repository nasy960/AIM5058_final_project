{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import json\n",
    "import numpy as np\n",
    "\n",
    "# import nltk\n",
    "# from nltk.corpus import stopwords\n",
    "import spacy\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "from scipy.stats import gamma, poisson\n",
    "from scipy.special import logsumexp\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# set parameter\n",
    "def make_config():\n",
    "    config = {}\n",
    "    ##### for option #####\n",
    "    config[\"START_IDX\"] = 50\n",
    "    config[\"N_WORDS\"] = 2000\n",
    "    config[\"DEFAULT_DATA_PATH\"] = \"datasets/ap\"\n",
    "    config[\"FORCE_LOAD_FROM_RAW_DATA\"] = False\n",
    "    config[\"ALPHA\"] = 5e+1\n",
    "    config[\"BETA\"] = 1e-2\n",
    "    config[\"N_CLS\"] = 5\n",
    "    config[\"N_ITER\"] = 100\n",
    "    config[\"N_TOPK\"] = 10\n",
    "    #############################\n",
    "    return config\n",
    "\n",
    "\n",
    "class ApData:\n",
    "    def __init__(self, start_idx, n_words, data_path):\n",
    "\n",
    "        # # for stop word download\n",
    "        # nltk.download('stopwords')\n",
    "        # english_stopwords = stopwords.words('english')\n",
    "\n",
    "        # load custom stopwords (nltk + person name + etc..)\n",
    "        english_stopwords = []\n",
    "        with open(\"stopwords/stopwords.txt\", \"rt\", encoding='UTF8') as f:\n",
    "            lines = f.readlines()\n",
    "            for line in lines:\n",
    "                english_stopwords.append(line.replace(\"\\n\",\"\"))\n",
    "\n",
    "        # read vocabulary\n",
    "        with open(data_path+\"/\"+\"vocab.txt\",'r') as f:\n",
    "            raw_vocabs = f.read().splitlines()\n",
    "\n",
    "        # check frequency\n",
    "        freq_raw_dict = {}\n",
    "        with open(data_path+\"/\"+\"ap.dat\",'r') as f:\n",
    "            lines = f.readlines()\n",
    "            for line in lines:\n",
    "                freqs = line.split(' ')[1:]\n",
    "                for i in range(len(freqs)):\n",
    "                    key, val = int(freqs[i].split(':')[0]),int(freqs[i].split(':')[1])\n",
    "                    if raw_vocabs[key] not in english_stopwords:\n",
    "                        if key not in freq_raw_dict.keys():\n",
    "                            freq_raw_dict[key] = val\n",
    "                        else:\n",
    "                            freq_raw_dict[key] += val\n",
    "        freq_raw_dict = sorted(freq_raw_dict.items(), key=lambda x:x[1], reverse=True)\n",
    "        \n",
    "        # check for validity\n",
    "        if freq_raw_dict[start_idx + n_words][1] == 0:\n",
    "            raise Exception(\"Invalid start idx and n words!!!\")\n",
    "        \n",
    "        valid_idx = [i for i,_ in freq_raw_dict[start_idx:start_idx+n_words]]\n",
    "        valid_idx_dict = {idx:i for i,idx in enumerate(valid_idx)}\n",
    "\n",
    "        # make matrix of data : row idx -> article num, col idx -> valid word\n",
    "        preprocessed_data = []\n",
    "        with open(data_path+\"/\"+\"ap.dat\",'r') as f:\n",
    "            lines = f.readlines()\n",
    "            for line in lines:\n",
    "                freq_dicts = line.split(' ')[1:]\n",
    "                row_data = np.zeros(shape=n_words)\n",
    "                for _, freq_dict in enumerate(freq_dicts):\n",
    "                    key, val = int(freq_dict.split(':')[0]),int(freq_dict.split(':')[1])\n",
    "                    if key in valid_idx:\n",
    "                        row_data[valid_idx_dict[key]] = val\n",
    "                preprocessed_data.append(row_data)\n",
    "        preprocessed_data = np.vstack(preprocessed_data)\n",
    "\n",
    "        # for save\n",
    "        self.raw_vocabs = raw_vocabs\n",
    "        self.preprocessed_data = preprocessed_data\n",
    "        self.valid_idx = valid_idx\n",
    "        self.n_words = n_words\n",
    "        self.n_docs = preprocessed_data.shape[0]\n",
    "\n",
    "\n",
    "\n",
    "class GibbsSampler:\n",
    "    def __init__(self, data, X, n_cls, n_topK=10):\n",
    "        self.data = data\n",
    "        self.X = X\n",
    "        self.n_cls = n_cls\n",
    "        self.n_doc = X.shape[0]\n",
    "        self.n_word = X.shape[1]\n",
    "        self.K = n_topK\n",
    "\n",
    "    def init_label(self, alpha=1, beta=1, opt=\"uniform_random\"):\n",
    "        # init alpha and beta using hyperparameter\n",
    "        self.Alpha = np.ones(shape=(self.n_cls, self.n_word)) * alpha\n",
    "        self.Beta = np.ones(shape=(self.n_cls, self.n_word)) * beta\n",
    "\n",
    "        # method for initialize label\n",
    "        if opt==\"uniform_random\":\n",
    "            self.Z = np.random.randint(self.n_cls, size=(self.n_doc, 1))\n",
    "\n",
    "        elif opt==\"kmeans\":\n",
    "            kmeans = KMeans(n_clusters=self.n_cls, random_state=0).fit(self.X)\n",
    "            self.Z = kmeans.labels_.astype(int)\n",
    "\n",
    "        else:\n",
    "            raise Exception(str(\"There is no initialize option named \\\"\") + opt + \"\\\"!\")\n",
    "        \n",
    "        # pre-calculating increment of alpha and beta for alpha hat and beta hat\n",
    "        self.alpha_hat = np.zeros_like(self.Alpha)\n",
    "        self.beta_hat = np.zeros_like(self.Beta)\n",
    "        for doc, label in zip(self.X, self.Z):\n",
    "            self.alpha_hat[int(label)] += doc\n",
    "            self.beta_hat[int(label)] += np.where(doc != 0, 1, 0)\n",
    "\n",
    "        \n",
    "    def sampling(self, n_iter):\n",
    "        for n in tqdm(range(n_iter)):\n",
    "            for doc_idx in tqdm(range(self.n_doc)):\n",
    "                doc = self.X[doc_idx]\n",
    "                label_prev = self.Z[doc_idx]\n",
    "                alpha_hat = self.alpha_hat + self.Alpha\n",
    "                beta_hat = self.beta_hat + self.Beta\n",
    "\n",
    "                lambdas = np.random.gamma(shape=alpha_hat, scale=np.divide(1, beta_hat))\n",
    "                self.lambdas = lambdas\n",
    "                prior_log = gamma.logpdf(lambdas, self.Alpha, self.Beta)\n",
    "                likelihood_log = poisson.logpmf(doc, lambdas)\n",
    "                posterior_log = likelihood_log + prior_log   # shape = (n_cls, n_word)\n",
    "                posterior_log = np.sum(posterior_log, axis=1)   # shape = (n_word,)\n",
    "\n",
    "                ## original case --> nan\n",
    "                # posterior_log -= np.min(posterior_log)\n",
    "                # posterior = np.exp(posterior_log)\n",
    "                # w = posterior / np.sum(posterior)\n",
    "\n",
    "                # using log scale calculation because of precision\n",
    "                aa = logsumexp(posterior_log)\n",
    "                posterior_log -= aa\n",
    "                w = np.exp(posterior_log)\n",
    "                w = w / np.sum(w)\n",
    "\n",
    "                z_new = np.random.multinomial(1, w)\n",
    "                label_new = np.where(z_new==1)[0]\n",
    "                self.Z[doc_idx] = label_new\n",
    "\n",
    "                self.alpha_hat[label_prev] -= doc\n",
    "                self.alpha_hat[label_new] += doc\n",
    "                self.beta_hat[label_prev] -= np.where(doc != 0, 1, 0)\n",
    "                self.beta_hat[label_new] += np.where(doc != 0, 1, 0)\n",
    "\n",
    "            topK_idx = self.get_topK_words()\n",
    "            res = {}\n",
    "            for i in range(self.n_cls):\n",
    "                idx = topK_idx[i]\n",
    "                res[i] = []\n",
    "                for j in idx:\n",
    "                    res[i].append(self.data.raw_vocabs[self.data.valid_idx[j]])\n",
    "\n",
    "            write_json(res, \"top_word_iter\"+str(n)+\".json\")\n",
    "\n",
    "    \n",
    "    def get_topK_words(self):\n",
    "        topK_idx = []\n",
    "        for i in range(self.n_cls):\n",
    "            sorted_idx = np.argsort(self.lambdas[i])[::-1]\n",
    "            topK_idx.append(sorted_idx[:self.K])\n",
    "\n",
    "        return topK_idx\n",
    "\n",
    "\n",
    "\n",
    "def check_save_and_load(config):\n",
    "    start_idx = config[\"START_IDX\"]\n",
    "    n_words = config[\"N_WORDS\"]\n",
    "    force_load_raw_data = config[\"FORCE_LOAD_FROM_RAW_DATA\"]\n",
    "    data_path = config[\"DEFAULT_DATA_PATH\"]\n",
    "\n",
    "    file_name = \"preprocessed_data/data_\" + str(start_idx) + \"_\" + str(n_words) + \".pickle\"\n",
    "    if os.path.isfile(file_name) and not force_load_raw_data:\n",
    "        print(\"##### load from save file( \"+ file_name +\" )!\")\n",
    "        with open(file_name,'rb') as fr:\n",
    "            data = pickle.load(fr)\n",
    "    else:\n",
    "        print(\"##### load from raw data!\")\n",
    "        data = ApData(start_idx, n_words, data_path)\n",
    "        with open(file_name, 'wb') as fw:\n",
    "            pickle.dump(data, fw)\n",
    "    print(\"##### load finish!\")\n",
    "    return data\n",
    "\n",
    "\n",
    "def write_json(dictionary, json_file=\"result.json\"):\n",
    "    file_name = \"result/\" + json_file\n",
    "    with open(file_name, 'w') as f:\n",
    "        json.dump(dictionary, f)\n",
    "\n",
    "\n",
    "config = make_config()\n",
    "data = check_save_and_load(config)\n",
    "\n",
    "res = {}\n",
    "res[\"valid_word\"] = []\n",
    "for idx in data.valid_idx:\n",
    "    res[\"valid_word\"].append(data.raw_vocabs[idx])\n",
    "write_json(res, \"valid_word.json\")\n",
    "\n",
    "sampler = GibbsSampler(data, data.preprocessed_data, config[\"N_CLS\"], config[\"N_TOPK\"])\n",
    "sampler.init_label(config[\"ALPHA\"], config[\"BETA\"], \"kmeans\")\n",
    "sampler.sampling(config[\"N_ITER\"])"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
